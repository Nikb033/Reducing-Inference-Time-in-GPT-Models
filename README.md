# ğŸ§  Reducing Inference Time in GPT Models

This project benchmarks attention mechanism optimizationsâ€”**FlashAttention**, **ALiBi (Attention with Linear Biases)**, and **Sparse Attention**â€”within the GPT-2 architecture to reduce inference time while preserving summarization quality.

Built using **PyTorch** and **HuggingFace Transformers**, the model is evaluated on the **News Dataset** for summarization, with comparisons across latency, ROUGE/BLEU scores, and GPU memory usage.

---

## ğŸš€ Key Features

- ğŸ” **FlashAttention 2.4**: GPU-optimized attention kernel for speed and efficiency  
- ğŸ§­ **ALiBi**: Adds linear bias for positional awareness and long-context generalization  
- ğŸ“‰ **Sparse Attention**: Sliding window pattern to reduce attention computation  
- ğŸ§ª **Benchmarked** on Colab Pro A100 with consistent 20.9GB GPU usage  
- ğŸ“Š **Evaluation Metrics**: ROUGE-1/2/L, BLEU, inference latency, GPU memory

---

## ğŸ“‚ Repository Structure

