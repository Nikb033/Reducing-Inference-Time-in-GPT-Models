# Reducing-Inference-Time-in-GPT-Models
This project benchmarks FlashAttention, ALiBi, and Sparse Attention within GPT-2 to reduce inference time while preserving summarization quality. Built with PyTorch and HuggingFace, it evaluates ROUGE, BLEU, latency, and GPU memory usage across optimized attention variants.
