# ğŸ§  Reducing Inference Time in GPT Models

This project benchmarks attention mechanism optimizationsâ€”**FlashAttention**, **ALiBi (Attention with Linear Biases)**, and **Sparse Attention**â€”within the GPT-2 architecture to reduce inference time while preserving summarization quality.

Built using **PyTorch** and **HuggingFace Transformers**, the model is evaluated on the **News Dataset** for summarization, with comparisons across latency, ROUGE/BLEU scores, and GPU memory usage.

---

## ğŸš€ Key Features

- ğŸ” **FlashAttention 2.4**: GPU-optimized attention kernel for speed and efficiency  
- ğŸ§­ **ALiBi**: Adds linear bias for positional awareness and long-context generalization  
- ğŸ“‰ **Sparse Attention**: Sliding window pattern to reduce attention computation  
- ğŸ§ª **Benchmarked** on Colab Pro A100 with competetive GPU usage  
- ğŸ“Š **Evaluation Metrics**: ROUGE-1/2/L, BLEU, inference latency, GPU memory

---

## ğŸ“‚ Repository Structure
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ gpt2_flash.py # FlashAttention integration
â”‚ â”œâ”€â”€ gpt2_alibi.py # ALiBi positional bias logic
â”‚ â””â”€â”€ gpt2_sparse.py # Sparse attention window mask
â”œâ”€â”€ data/
â”‚ â””â”€â”€ news_dataset.py # Preprocessing logic for News Summary dataset
â”œâ”€â”€ evaluate/
â”‚ â””â”€â”€ metrics.py # ROUGE, BLEU, and timing functions
â”œâ”€â”€ hpml_final.ipynb # Full training + evaluation notebook
â”œâ”€â”€ plots/ # Performance visualizations
â””â”€â”€ README.md


---

## ğŸ“ˆ Results Summary

| Variant           | ROUGE-L | BLEU   | Inference Time (ms) | Memory (GB) |
|------------------|---------|--------|----------------------|-------------|
| Standard GPT-2    | 0.1215  | 0.0263 | 96.22                | ~20.9       |
| Flash GPT-2       | 0.1076  | 0.0221 | 73.42                | ~20.9       |
| Flash + ALiBi     | 0.1320  | 0.0295 | 87.08                | ~20.9       |
| Flash + Sparse    | 0.1303  | 0.0282 | 86.86                | ~20.9       |

---

bash--
jupyter notebook hpml_final.ipynb

ğŸ™Œ Acknowledgements
FlashAttention by Tri Dao et al.

ALiBi by Press et al.

HuggingFace Transformers and Datasets





